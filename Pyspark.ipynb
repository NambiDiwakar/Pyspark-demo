{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.1.2-bin-hadoop2.7'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"how to read csv file\") \\\n",
    "    .getOrCreate()\n",
    "path = \"C:\\\\Users\\\\nambi\\\\OneDrive\\\\Desktop\\\\Internship\\\\transactions.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- date_of_purchase: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "def flatten(df):\n",
    "    complex_fields = dict([\n",
    "        (field.name, field.dataType) \n",
    "        for field in df.schema.fields \n",
    "        if isinstance(field.dataType, T.ArrayType) or isinstance(field.dataType, T.StructType)\n",
    "    ])\n",
    "    \n",
    "    qualify = list(complex_fields.keys())[0] + \"_\"\n",
    "\n",
    "    while len(complex_fields) != 0:\n",
    "        col_name = list(complex_fields.keys())[0]\n",
    "        \n",
    "        if isinstance(complex_fields[col_name], T.StructType):\n",
    "            expanded = [F.col(col_name + '.' + k).alias(col_name + '_' + k) \n",
    "                        for k in [ n.name for n in  complex_fields[col_name]]\n",
    "                       ]\n",
    "            \n",
    "            df = df.select(\"*\", *expanded).drop(col_name)\n",
    "    \n",
    "        elif isinstance(complex_fields[col_name], T.ArrayType): \n",
    "            df = df.withColumn(col_name, F.explode(col_name))\n",
    "    \n",
    "      \n",
    "        complex_fields = dict([\n",
    "            (field.name, field.dataType)\n",
    "            for field in df.schema.fields\n",
    "            if isinstance(field.dataType, T.ArrayType) or isinstance(field.dataType, T.StructType)\n",
    "        ])\n",
    "        \n",
    "        \n",
    "    for df_col_name in df.columns:\n",
    "        df = df.withColumnRenamed(df_col_name, df_col_name.replace(qualify, \"\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "df = spark.read.json(path)\n",
    "#peopleDF.show()\n",
    "df_trans=flatten(df)\n",
    "df_trans.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------+\n",
      "|product_id|purchase_count_per_product_id|\n",
      "+----------+-----------------------------+\n",
      "|       P13|                            2|\n",
      "|       P23|                            2|\n",
      "|        P2|                            2|\n",
      "|       P15|                            2|\n",
      "|       P11|                            2|\n",
      "|        P7|                            2|\n",
      "|       P21|                            2|\n",
      "|        P8|                            2|\n",
      "|       P14|                            2|\n",
      "|        P3|                            2|\n",
      "|       P10|                            2|\n",
      "|       P16|                            2|\n",
      "|        P4|                            2|\n",
      "|        P1|                            1|\n",
      "|        P5|                            2|\n",
      "|       P17|                            2|\n",
      "|       P12|                            2|\n",
      "|       P22|                            2|\n",
      "|       P19|                            2|\n",
      "|       P24|                            1|\n",
      "|       P18|                            2|\n",
      "|        P6|                            2|\n",
      "|        P9|                            2|\n",
      "|       P20|                            2|\n",
      "+----------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_trans['purchase_count_per_product_id']=df_trans.withColumn()\n",
    "#df_trans = \n",
    "df_purchase = df_trans.groupBy(\"product_id\") \\\n",
    "    .agg(count(\"product_id\").alias(\"purchase_count_per_product_id\"))\n",
    "df_purchase.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|customer_id|loyalty_score|\n",
      "+-----------+-------------+\n",
      "|         C1|         0.67|\n",
      "|         C2|         0.78|\n",
      "|         C3|         0.81|\n",
      "|         C4|         0.46|\n",
      "|         C5|         0.98|\n",
      "|         C6|         0.78|\n",
      "|         C7|         0.67|\n",
      "|         C8|         0.99|\n",
      "|         C9|         0.88|\n",
      "|        C10|         0.23|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customer = spark.read.csv('C:\\\\Users\\\\nambi\\\\OneDrive\\\\Desktop\\\\Internship\\\\customers.csv',header=True)\n",
    "df_customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----------------+\n",
      "|product_id|product_description|product_category|\n",
      "+----------+-------------------+----------------+\n",
      "|        P1|              Paste|         Grocery|\n",
      "|        P2|              Apple|          Fruits|\n",
      "|        P3|               Kiwi|          Fruits|\n",
      "|        P4|      Hide and seek|        Biscuits|\n",
      "|        P5|              Brush|         Grocery|\n",
      "|        P6|                Dal|          Grains|\n",
      "|        P7|              Wheat|          Grains|\n",
      "|        P8|          Bournvita|         Grocery|\n",
      "|        P9|               Mask|         Grocery|\n",
      "|       P10|        Nail cutter|         Grocery|\n",
      "|       P11|              Boost|         Grocery|\n",
      "|       P12|            Cabbage|          Vegies|\n",
      "|       P13|             Carrot|          Vegies|\n",
      "|       P14|             Banana|          Fruits|\n",
      "|       P15|               Eggs|         Grocery|\n",
      "|       P16|            Chicken|            Meat|\n",
      "|       P17|             Tomato|          Vegies|\n",
      "|       P18|               Soap|         Grocery|\n",
      "|       P19|               Beef|            Meat|\n",
      "|       P20|               Oats|          Grains|\n",
      "+----------+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_prod=spark.read.csv('C:\\\\Users\\\\nambi\\\\OneDrive\\\\Desktop\\\\Internship\\\\Products.csv',header=True)\n",
    "df_prod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=df_trans.join(df_customer,[\"customer_id\"]) \\\n",
    "     .join(df_prod,[\"product_id\"]) \\\n",
    "     .join(df_purchase,[\"product_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "|customer_id|loyalty_score|product_id|product_category|purchase_count_per_product_id|\n",
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "|         C1|         0.67|        P1|         Grocery|                            1|\n",
      "|         C1|         0.67|        P2|          Fruits|                            2|\n",
      "|         C2|         0.78|        P2|          Fruits|                            2|\n",
      "|         C2|         0.78|        P3|          Fruits|                            2|\n",
      "|         C3|         0.81|        P3|          Fruits|                            2|\n",
      "|         C3|         0.81|        P4|        Biscuits|                            2|\n",
      "|         C4|         0.46|        P4|        Biscuits|                            2|\n",
      "|         C4|         0.46|        P5|         Grocery|                            2|\n",
      "|         C5|         0.98|        P5|         Grocery|                            2|\n",
      "|         C5|         0.98|        P6|          Grains|                            2|\n",
      "|         C6|         0.78|        P6|          Grains|                            2|\n",
      "|         C6|         0.78|        P7|          Grains|                            2|\n",
      "|         C7|         0.67|        P7|          Grains|                            2|\n",
      "|         C7|         0.67|        P8|         Grocery|                            2|\n",
      "|         C8|         0.99|        P8|         Grocery|                            2|\n",
      "|         C8|         0.99|        P9|         Grocery|                            2|\n",
      "|         C9|         0.88|        P9|         Grocery|                            2|\n",
      "|         C9|         0.88|       P10|         Grocery|                            2|\n",
      "|        C10|         0.23|       P10|         Grocery|                            2|\n",
      "|        C10|         0.23|       P11|         Grocery|                            2|\n",
      "+-----------+-------------+----------+----------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final2 = df_final[[\"customer_id\",\"loyalty_score\",\"product_id\",\"product_category\",\"purchase_count_per_product_id\"]]\n",
    "df_final2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
